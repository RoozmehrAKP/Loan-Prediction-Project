{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model no.1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.8426966292134831\n",
      "Confusion Matrix:\n",
      " [[17 19]\n",
      " [ 9 75]]\n",
      "False Positives (FP): 19\n",
      "False Negatives (FN): 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#  Data loading\n",
    "train_data = pd.read_csv(r'd:\\DaneshKar\\Project_Eden\\Clean_Loan.csv')\n",
    "test_data = pd.read_csv(r'd:\\DaneshKar\\Project_Eden\\Clean_Test.csv')\n",
    "\n",
    "#  Data preprocessing (handling missing values, scaling features)\n",
    "train_data = train_data.dropna()  # حذف مقادیر گمشده برای ساده‌سازی\n",
    "X = train_data.drop('Loan_Status', axis=1)  # انتخاب ویژگی‌ها\n",
    "y = train_data['Loan_Status']  # انتخاب هدف (loan_status)\n",
    "\n",
    "#  Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#  Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#  Model - Random Forest\n",
    "model = RandomForestClassifier(random_state=42, class_weight='balanced')  # استفاده از class_weight برای توازن داده‌ها\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#  Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#  Evaluation\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# بررسی تعداد false positives (FP) و false negatives (FN)\n",
    "fp = conf_matrix[0, 1]  # تعداد false positive\n",
    "fn = conf_matrix[1, 0]  # تعداد false negative\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model No.2 : Rf with threshold for decreasing FalsePositive*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New F1 Score:  0.7975460122699386\n",
      "New Confusion Matrix:\n",
      " [[22 14]\n",
      " [19 65]]\n",
      "False Positives (FP) with new threshold: 14\n",
      "False Negatives (FN) with new threshold: 19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#  Getting predicted probabilities instead of discrete labels\n",
    "y_prob = model.predict_proba(X_test)[:, 1]  # Probability for the positive class (loan_status=1)\n",
    "\n",
    "#  Define new threshold (e.g. 0.7)\n",
    "threshold = 0.7\n",
    "y_pred_new = (y_prob >= threshold).astype(int)  # Applying the new threshold\n",
    "\n",
    "#  Evaluation with new threshold\n",
    "f1_new = f1_score(y_test, y_pred_new)\n",
    "conf_matrix_new = confusion_matrix(y_test, y_pred_new)\n",
    "\n",
    "print(\"New F1 Score: \", f1_new)\n",
    "print(\"New Confusion Matrix:\\n\", conf_matrix_new)\n",
    "\n",
    "# Checking False Positives and False Negatives again\n",
    "fp_new = conf_matrix_new[0, 1]  # False Positives (FP)\n",
    "fn_new = conf_matrix_new[1, 0]  # False Negatives (FN)\n",
    "print(f\"False Positives (FP) with new threshold: {fp_new}\")\n",
    "print(f\"False Negatives (FN) with new threshold: {fn_new}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model No.3 : Rf with balance data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with SMOTE:  0.8372093023255814\n",
      "Confusion Matrix with SMOTE:\n",
      " [[20 16]\n",
      " [12 72]]\n",
      "False Positives (FP) with SMOTE: 16\n",
      "False Negatives (FN) with SMOTE: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the data\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train the model again with the balanced data\n",
    "model_smote = RandomForestClassifier(random_state=42)\n",
    "model_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "#  Predictions\n",
    "y_pred_smote = model_smote.predict(X_test)\n",
    "\n",
    "# Evaluation with SMOTE balance\n",
    "f1_smote = f1_score(y_test, y_pred_smote)\n",
    "conf_matrix_smote = confusion_matrix(y_test, y_pred_smote)\n",
    "\n",
    "print(\"F1 Score with SMOTE: \", f1_smote)\n",
    "print(\"Confusion Matrix with SMOTE:\\n\", conf_matrix_smote)\n",
    "\n",
    "# بررسی تعداد false positives (FP) و false negatives (FN)\n",
    "fp_smote = conf_matrix_smote[0, 1]\n",
    "fn_smote = conf_matrix_smote[1, 0]\n",
    "print(f\"False Positives (FP) with SMOTE: {fp_smote}\")\n",
    "print(f\"False Negatives (FN) with SMOTE: {fn_smote}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model No.4 : Rf with threshold and balance data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New F1 Score:  0.8\n",
      "New Confusion Matrix:\n",
      " [[21 15]\n",
      " [18 66]]\n",
      "False Positives (FP) with new threshold: 15\n",
      "False Negatives (FN) with new threshold: 18\n"
     ]
    }
   ],
   "source": [
    "#  Getting predicted probabilities instead of discrete labels\n",
    "y_prob_new = model_smote.predict_proba(X_test)[:, 1]  # Probability for the positive class (loan_status=1)\n",
    "\n",
    "#  Define new threshold (e.g. 0.6)\n",
    "threshold = 0.6\n",
    "y_pred_smote_new = (y_prob_new >= threshold).astype(int)  # Applying the new threshold\n",
    "\n",
    "#  Evaluation with new threshold\n",
    "f1_smote_new = f1_score(y_test, y_pred_smote_new)\n",
    "conf_matrix_smote_new = confusion_matrix(y_test, y_pred_smote_new)\n",
    "\n",
    "print(\"New F1 Score: \", f1_smote_new)\n",
    "print(\"New Confusion Matrix:\\n\", conf_matrix_smote_new)\n",
    "\n",
    "# Checking False Positives and False Negatives again\n",
    "fp_new = conf_matrix_smote_new[0, 1]  # False Positives (FP)\n",
    "fn_new = conf_matrix_smote_new[1, 0]  # False Negatives (FN)\n",
    "print(f\"False Positives (FP) with new threshold: {fp_new}\")\n",
    "print(f\"False Negatives (FN) with new threshold: {fn_new}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *calculate error percentage*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rf Model No.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Error: 23.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "error_percentage1 = 100 * (1 - accuracy)\n",
    "print(\"Percentage of Error:\", error_percentage1, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rf Model No.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Error: 27.500000000000004 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_new)\n",
    "error_percentage2 = 100 * (1 - accuracy)\n",
    "print(\"Percentage of Error:\", error_percentage2, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rf Model No.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Error: 23.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_smote)\n",
    "error_percentage3 = 100 * (1 - accuracy)\n",
    "print(\"Percentage of Error:\", error_percentage3, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rf Model No.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Error: 27.500000000000004 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_smote_new)\n",
    "error_percentage4 = 100 * (1 - accuracy)\n",
    "print(\"Percentage of Error:\", error_percentage4, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest Model Evaluation and Selection\n",
    "\n",
    "Based on the provided criteria (**F1 Score**, **Percentage of Error**, and minimizing **False Positives (FP)**), here's an analysis of the four models:\n",
    "\n",
    "## Summary of Metrics\n",
    "\n",
    "| **Model**  | **F1 Score** | **FP** | **FN** | **Error %**  |\n",
    "|------------|--------------|---------|---------|--------------|\n",
    "| **Model 1** | 0.8427       | 19      | 9       | 23.33        |\n",
    "| **Model 2** | 0.7975       | 14      | 19      | 27.50        |\n",
    "| **Model 3** | 0.8372       | 16      | 12      | 23.33        |\n",
    "| **Model 4** | 0.8000       | 15      | 18      | 27.50        |\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis\n",
    "\n",
    "### Model 1\n",
    "- **F1 Score**: The highest (0.8427), indicating an excellent balance between Precision and Recall.\n",
    "- **Error %**: Good (23.33%).\n",
    "- **FP**: The highest among all models (19), which is unfavorable for minimizing false positives.\n",
    "\n",
    "### Model 2\n",
    "- **FP**: The lowest (14), aligning with the goal of minimizing false positives.\n",
    "- **F1 Score**: The lowest (0.7975), showing weaker Precision and Recall balance compared to other models.\n",
    "- **Error %**: The highest (27.50%), which is a disadvantage.\n",
    "\n",
    "### Model 3\n",
    "- **F1 Score**: Near the top (0.8372, slightly below Model 1) and significantly better than Models 2 and 4.\n",
    "- **Error %**: Matches Model 1 (23.33%) and is better than Models 2 and 4.\n",
    "- **FP**: Lower than Model 1 (16 vs. 19) but slightly higher than Model 2 and Model 4.\n",
    "\n",
    "### Model 4\n",
    "- **FP**: Moderate (15), lower than Models 1 and 3 but slightly higher than Model 2.\n",
    "- **F1 Score**: Better than Model 2 (0.8000 vs. 0.7975) but falls behind Models 1 and 3.\n",
    "- **Error %**: Same as Model 2 (27.50%), which is higher than Models 1 and 3.\n",
    "\n",
    "---\n",
    "\n",
    "## Best Model Selection\n",
    "\n",
    "### Recommendation:\n",
    "- **Model 3** remains the best option based on:\n",
    "  - A strong balance between **F1 Score** (0.8372) and **Error %** (23.33%).\n",
    "  - Moderately low **FP** (16), which improves upon Model 1's performance while maintaining a strong overall metric balance.\n",
    "\n",
    "### Alternative:\n",
    "- If minimizing **FP** is the highest priority, **Model 2** can be considered due to its lowest **FP** (14). However, the drop in **F1 Score** and higher **Error %** make it less favorable for a balanced solution.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list.append({\n",
    "    'Random Forest Error Rate (%)': error_percentage3,\n",
    "    'Random Forest f1-score (%)': f1_smote,\n",
    "    'Random Forest FP (%)' : \"16\"\n",
    "})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model No.1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with XGBoost:  0.8235294117647058\n",
      "Confusion Matrix with XGBoost:\n",
      " [[20 16]\n",
      " [14 70]]\n",
      "False Positives (FP) with XGBoost: 16\n",
      "False Negatives (FN) with XGBoost: 14\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# Calculate the number of samples per class\n",
    "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Train an XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42, scale_pos_weight=class_counts[0] / class_counts[1])\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "#  Predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"F1 Score with XGBoost: \", f1_xgb)\n",
    "print(\"Confusion Matrix with XGBoost:\\n\", conf_matrix_xgb)\n",
    "\n",
    "# False Positives and False Negatives\n",
    "fp_xgb = conf_matrix_xgb[0, 1]\n",
    "fn_xgb = conf_matrix_xgb[1, 0]\n",
    "print(f\"False Positives (FP) with XGBoost: {fp_xgb}\")\n",
    "print(f\"False Negatives (FN) with XGBoost: {fn_xgb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Error: 25.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "error_percentage_xgb1 = 100 * (1 - accuracy)\n",
    "print(\"Percentage of Error:\", error_percentage_xgb1, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model No.2 : XGBoost with balance data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with XGBoost:  0.7951807228915663\n",
      "Confusion Matrix with XGBoost:\n",
      " [[20 16]\n",
      " [18 66]]\n",
      "False Positives (FP) with XGBoost: 16\n",
      "False Negatives (FN) with XGBoost: 18\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# Train an XGBoost model\n",
    "xgb_model2 = xgb.XGBClassifier(random_state=42, eval_metric='auc')\n",
    "xgb_model2.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb2 = xgb_model2.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "f1_xgb_balance = f1_score(y_test, y_pred_xgb2)\n",
    "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb2)\n",
    "\n",
    "print(\"F1 Score with XGBoost: \", f1_xgb_balance)\n",
    "print(\"Confusion Matrix with XGBoost:\\n\", conf_matrix_xgb)\n",
    "\n",
    "# False Positives and False Negatives\n",
    "fp_xgb = conf_matrix_xgb[0, 1]\n",
    "fn_xgb = conf_matrix_xgb[1, 0]\n",
    "print(f\"False Positives (FP) with XGBoost: {fp_xgb}\")\n",
    "print(f\"False Negatives (FN) with XGBoost: {fn_xgb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Error: 28.333333333333332 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_xgb2)\n",
    "error_percentage_xgb2 = 100 * (1 - accuracy)\n",
    "print(\"Percentage of Error:\", error_percentage_xgb2, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST Model Evaluation and Selection\n",
    "\n",
    "Based on the provided criteria (**F1 Score**, **Percentage of Error**, and minimizing **False Positives (FP)**), here is an analysis of the two models.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Metrics\n",
    "\n",
    "| **Model**  | **F1 Score** | **FP** | **FN** | **Error %** |\n",
    "|------------|--------------|---------|---------|-------------|\n",
    "| **Model 1** | 0.8235       | 16      | 14      | 25.0        |\n",
    "| **Model 2** | 0.7952       | 16      | 18      | 28.33       |\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis\n",
    "\n",
    "### **Model 1**:\n",
    "1. **F1 Score**:\n",
    "   - The highest (0.8235), indicating a better balance between Precision and Recall.\n",
    "2. **Error %**:\n",
    "   - Lower (25.0%) compared to Model 2, indicating higher accuracy.\n",
    "3. **FP**:\n",
    "   - Equal to Model 2 (16).\n",
    "4. **FN**:\n",
    "   - Lower (14 vs. 18), reflecting better Recall performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model 2**:\n",
    "1. **F1 Score**:\n",
    "   - Lower than Model 1 (0.7952 vs. 0.8235), showing weaker Precision and Recall balance.\n",
    "2. **Error %**:\n",
    "   - Higher than Model 1 (28.33% vs. 25.0%).\n",
    "3. **FP**:\n",
    "   - Equal to Model 1 (16).\n",
    "4. **FN**:\n",
    "   - Higher (18 vs. 14), indicating a drop in Recall.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion and Recommendation:\n",
    "- **Model 1** is the best choice because:\n",
    "  - It has the highest **F1 Score** (0.8235 vs. 0.7952).\n",
    "  - It has a lower **Error Percentage** (25.0% vs. 28.33%).\n",
    "  - It has fewer **False Negatives (FN)**, resulting in better Recall.\n",
    "\n",
    "Since the goal is to **minimize False Positives (FP)**, and both models have the same FP value (16), **Model 1** is the recommended choice due to its overall better performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Note:\n",
    "If further optimization for **reducing FP** is required, the decision threshold for **Model 1** can be adjusted to improve this metric further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list.append({\n",
    "    'XGBOOST Error Rate (%)': error_percentage_xgb1,\n",
    "    'XGBOOST f1-score (%)': f1_xgb,\n",
    "    'XGBOOST FP (%)' : \"16\"\n",
    "})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # LogesticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model No.1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Logistic Regression):  0.861878453038674\n",
      "Confusion Matrix (Logistic Regression):\n",
      " [[17 19]\n",
      " [ 6 78]]\n",
      "False Positives (FP) (Logistic Regression): 19\n",
      "False Negatives (FN) (Logistic Regression): 6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg_model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_log_reg = log_reg_model.predict(X_test)  # پیش‌بینی نتایج\n",
    "\n",
    "# Evaluation\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg)  # محاسبه F1 Score\n",
    "conf_matrix_log_reg = confusion_matrix(y_test, y_pred_log_reg)  # محاسبه Confusion Matrix\n",
    "\n",
    "print(\"F1 Score (Logistic Regression): \", f1_log_reg)  # نمایش F1 Score\n",
    "print(\"Confusion Matrix (Logistic Regression):\\n\", conf_matrix_log_reg)  # نمایش ماتریس گیجی\n",
    "\n",
    "# بررسی تعداد false positives (FP) و false negatives (FN)\n",
    "fp_log_reg = conf_matrix_log_reg[0, 1]  # تعداد false positive\n",
    "fn_log_reg = conf_matrix_log_reg[1, 0]  # تعداد false negative\n",
    "print(f\"False Positives (FP) (Logistic Regression): {fp_log_reg}\")\n",
    "print(f\"False Negatives (FN) (Logistic Regression): {fn_log_reg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Error: 20.833333333333336 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "error_percentage_log_reg = 100 * (1 - accuracy)\n",
    "print(\"Percentage of Error:\", error_percentage_log_reg, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model No.2 : LogesticRegression with balance data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Logistic Regression):  0.8461538461538461\n",
      "Confusion Matrix (Logistic Regression):\n",
      " [[15 21]\n",
      " [ 7 77]]\n",
      "False Positives (FP) (Logistic Regression): 21\n",
      "False Negatives (FN) (Logistic Regression): 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "log_reg_model2 = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "log_reg_model2.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predictions\n",
    "y_pred_log_reg = log_reg_model2.predict(X_test)  # پیش‌بینی نتایج\n",
    "\n",
    "# Evaluation\n",
    "f1_log_reg2 = f1_score(y_test, y_pred_log_reg)  # محاسبه F1 Score\n",
    "conf_matrix_log_reg = confusion_matrix(y_test, y_pred_log_reg)  # محاسبه Confusion Matrix\n",
    "\n",
    "print(\"F1 Score (Logistic Regression): \", f1_log_reg2)  # نمایش F1 Score\n",
    "print(\"Confusion Matrix (Logistic Regression):\\n\", conf_matrix_log_reg)  # نمایش ماتریس گیجی\n",
    "\n",
    "# بررسی تعداد false positives (FP) و false negatives (FN)\n",
    "fp_log_reg = conf_matrix_log_reg[0, 1]  # تعداد false positive\n",
    "fn_log_reg = conf_matrix_log_reg[1, 0]  # تعداد false negative\n",
    "print(f\"False Positives (FP) (Logistic Regression): {fp_log_reg}\")\n",
    "print(f\"False Negatives (FN) (Logistic Regression): {fn_log_reg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Error: 23.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "error_percentage_log_reg2 = 100 * (1 - accuracy)\n",
    "print(\"Percentage of Error:\", error_percentage_log_reg2, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogesticRegression Model Evaluation and Selection\n",
    "\n",
    "Based on the provided criteria (**F1 Score**, **Percentage of Error**, and minimizing **False Positives (FP)**), here is the analysis of the two models.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Metrics\n",
    "\n",
    "| **Model**  | **F1 Score** | **FP** | **FN** | **Error %** |\n",
    "|------------|--------------|---------|---------|-------------|\n",
    "| **Model 1** | 0.8619       | 19      | 6       | 20.83       |\n",
    "| **Model 2** | 0.8462       | 21      | 7       | 23.33       |\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis\n",
    "\n",
    "### **Model 1**:\n",
    "1. **F1 Score**:\n",
    "   - The highest (0.8619), indicating better overall performance.\n",
    "2. **Error %**:\n",
    "   - Lower (20.83%) compared to Model 2, indicating better accuracy.\n",
    "3. **FP**:\n",
    "   - Slightly better than Model 2 (19 vs. 21).\n",
    "4. **FN**:\n",
    "   - Lower (6 vs. 7), leading to better Recall.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model 2**:\n",
    "1. **F1 Score**:\n",
    "   - Lower than Model 1 (0.8462 vs. 0.8619), reflecting weaker Precision and Recall balance.\n",
    "2. **Error %**:\n",
    "   - Higher than Model 1 (23.33% vs. 20.83%).\n",
    "3. **FP**:\n",
    "   - Higher than Model 1 (21 vs. 19).\n",
    "4. **FN**:\n",
    "   - Slightly higher (7 vs. 6), resulting in slightly worse Recall.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion and Recommendation:\n",
    "- **Model 1** is the best choice because:\n",
    "  - It has the highest **F1 Score** (0.8619 vs. 0.8462).\n",
    "  - It has a lower **Error Percentage** (20.83% vs. 23.33%).\n",
    "  - It has fewer **False Positives (FP)** (19 vs. 21).\n",
    "  - It has fewer **False Negatives (FN)** (6 vs. 7).\n",
    "\n",
    "Model 1 performs better overall, especially in minimizing both False Positives and False Negatives.\n",
    "\n",
    "---\n",
    "\n",
    "### Note:\n",
    "If additional tuning for reducing **False Positives (FP)** is required, adjusting the model’s decision threshold may yield further improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list.append({\n",
    "    'Log_reg Error Rate (%)': error_percentage_log_reg,\n",
    "    'Log_reg f1_score (%)': f1_log_reg,\n",
    "    'Log_reg FP (%)' : \"19\"\n",
    "})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model No.1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (KNN):  0.8723404255319149\n",
      "Confusion Matrix (KNN):\n",
      " [[14 22]\n",
      " [ 2 82]]\n",
      "False Positives (FP) (KNN): 22\n",
      "False Negatives (FN) (KNN): 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_model = KNeighborsClassifier(n_neighbors=27)  # مقداردهی اولیه KNN\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_knn = knn_model.predict(X_test)  # پیش‌بینی نتایج\n",
    "\n",
    "# Evaluation\n",
    "f1_knn = f1_score(y_test, y_pred_knn)  # محاسبه F1 Score\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)  # محاسبه Confusion Matrix\n",
    "\n",
    "print(\"F1 Score (KNN): \", f1_knn)  # نمایش F1 Score\n",
    "print(\"Confusion Matrix (KNN):\\n\", conf_matrix_knn)  # نمایش ماتریس گیجی\n",
    "\n",
    "# بررسی تعداد false positives (FP) و false negatives (FN)\n",
    "fp_knn = conf_matrix_knn[0, 1]  # تعداد false positive\n",
    "fn_knn = conf_matrix_knn[1, 0]  # تعداد false negative\n",
    "print(f\"False Positives (FP) (KNN): {fp_knn}\")\n",
    "print(f\"False Negatives (FN) (KNN): {fn_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Error: 19.999999999999996 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "error_percentage_knn = 100 * (1 - accuracy)\n",
    "print(\"Percentage of Error:\", error_percentage_knn, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model No.2 : KNN with balance data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (KNN):  0.7636363636363637\n",
      "Confusion Matrix (KNN):\n",
      " [[18 18]\n",
      " [21 63]]\n",
      "False Positives (FP) (KNN): 18\n",
      "False Negatives (FN) (KNN): 21\n"
     ]
    }
   ],
   "source": [
    "knn_model2 = KNeighborsClassifier(n_neighbors=16)  # مقداردهی اولیه KNN\n",
    "knn_model2.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predictions\n",
    "y_pred_knn2 = knn_model2.predict(X_test)  # پیش‌بینی نتایج\n",
    "\n",
    "# Evaluation\n",
    "f1_knn2 = f1_score(y_test, y_pred_knn2)  # محاسبه F1 Score\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn2)  # محاسبه Confusion Matrix\n",
    "\n",
    "print(\"F1 Score (KNN): \", f1_knn2)  # نمایش F1 Score\n",
    "print(\"Confusion Matrix (KNN):\\n\", conf_matrix_knn)  # نمایش ماتریس گیجی\n",
    "\n",
    "# بررسی تعداد false positives (FP) و false negatives (FN)\n",
    "fp_knn = conf_matrix_knn[0, 1]  # تعداد false positive\n",
    "fn_knn = conf_matrix_knn[1, 0]  # تعداد false negative\n",
    "print(f\"False Positives (FP) (KNN): {fp_knn}\")\n",
    "print(f\"False Negatives (FN) (KNN): {fn_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Error: 32.49999999999999 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_knn2)\n",
    "error_percentage_knn2 = 100 * (1 - accuracy)\n",
    "print(\"Percentage of Error:\", error_percentage_knn2, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model No.3 : KNN with CV for best params* **params=best neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best number of neighbors (k): 16\n",
      "F1 Score (KNN with best k):  0.8663101604278075\n",
      "Confusion Matrix (KNN with best k):\n",
      " [[14 22]\n",
      " [ 3 81]]\n",
      "False Positives (FP) (KNN with best k): 22\n",
      "False Negatives (FN) (KNN with best k): 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "knn_model3 = KNeighborsClassifier()\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {'n_neighbors': range(1, 21)}  # جستجو برای تعداد همسایه از 1 تا 20\n",
    "grid_search = GridSearchCV(estimator=knn_model3, param_grid=param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameter (k)\n",
    "best_k = grid_search.best_params_['n_neighbors']\n",
    "print(f\"Best number of neighbors (k): {best_k}\")\n",
    "\n",
    "# Predictions using the best model\n",
    "best_knn_model = grid_search.best_estimator_  # مدل با بهترین k\n",
    "y_pred_knn3 = best_knn_model.predict(X_test)  # پیش‌بینی نتایج\n",
    "\n",
    "# Evaluation\n",
    "f1_knn3 = f1_score(y_test, y_pred_knn3)  # محاسبه F1 Score\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn3)  # محاسبه Confusion Matrix\n",
    "\n",
    "print(\"F1 Score (KNN with best k): \", f1_knn3)  # نمایش F1 Score\n",
    "print(\"Confusion Matrix (KNN with best k):\\n\", conf_matrix_knn)  # نمایش ماتریس گیجی\n",
    "\n",
    "# بررسی تعداد false positives (FP) و false negatives (FN)\n",
    "fp_knn = conf_matrix_knn[0, 1]  # تعداد false positive\n",
    "fn_knn = conf_matrix_knn[1, 0]  # تعداد false negative\n",
    "print(f\"False Positives (FP) (KNN with best k): {fp_knn}\")\n",
    "print(f\"False Negatives (FN) (KNN with best k): {fn_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Error: 20.833333333333336 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_knn3)\n",
    "error_percentage_knn3 = 100 * (1 - accuracy)\n",
    "print(\"Percentage of Error:\", error_percentage_knn3, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Model Evaluation and Selection\n",
    "\n",
    "Based on the provided criteria (**F1 Score**, **Percentage of Error**, and minimizing **False Positives (FP)**), here is the analysis of the three models.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Metrics\n",
    "\n",
    "| **Model**  | **F1 Score** | **FP** | **FN** | **Error %** |\n",
    "|------------|--------------|---------|---------|-------------|\n",
    "| **Model 1** | 0.8723       | 22      | 2       | 20.00       |\n",
    "| **Model 2** | 0.7636       | 18      | 21      | 32.50       |\n",
    "| **Model 3** | 0.8663       | 22      | 3       | 20.83       |\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis\n",
    "\n",
    "### **Model 1**:\n",
    "1. **F1 Score**:\n",
    "   - The highest (0.8723), indicating the best overall performance in balancing Precision and Recall.\n",
    "2. **Error %**:\n",
    "   - The lowest (20.00%), reflecting better accuracy compared to Model 2 and Model 3.\n",
    "3. **FP**:\n",
    "   - Slightly higher (22 vs. 18), compared to Model 2 but equal to Model 3.\n",
    "4. **FN**:\n",
    "   - The lowest (2 vs. 21 in Model 2, 3 in Model 3), indicating better Recall.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model 2**:\n",
    "1. **F1 Score**:\n",
    "   - The lowest (0.7636), indicating a weaker balance between Precision and Recall.\n",
    "2. **Error %**:\n",
    "   - The highest (32.50%), meaning lower overall accuracy compared to Models 1 and 3.\n",
    "3. **FP**:\n",
    "   - The lowest (18 vs. 22 in Models 1 and 3), which may be preferred when aiming to minimize False Positives.\n",
    "4. **FN**:\n",
    "   - The highest (21 vs. 2 in Model 1 and 3), leading to much poorer Recall.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model 3**:\n",
    "1. **F1 Score**:\n",
    "   - Slightly lower than Model 1 (0.8663 vs. 0.8723), but still high and a balanced performance between Precision and Recall.\n",
    "2. **Error %**:\n",
    "   - Slightly higher (20.83% vs. 20.00% in Model 1), but still a strong performance.\n",
    "3. **FP**:\n",
    "   - Equal to Model 1 (22).\n",
    "4. **FN**:\n",
    "   - Slightly higher (3 vs. 2 in Model 1), but still much better than Model 2.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion and Recommendation:\n",
    "- **Model 1** is the best choice because:\n",
    "  - It has the highest **F1 Score** (0.8723), indicating the best balance between Precision and Recall.\n",
    "  - It has the lowest **Error Percentage** (20.00%), reflecting the highest accuracy.\n",
    "  - Although it has slightly more **False Positives (FP)** compared to Model 2, it has significantly fewer **False Negatives (FN)**, making it preferable overall.\n",
    "\n",
    "---\n",
    "\n",
    "### Note:\n",
    "If further optimization for minimizing **False Positives (FP)** is required, you may experiment with the decision threshold or hyperparameter tuning. However, based on overall performance, **Model 1** is the best option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list.append({\n",
    "    'Log_reg Error Rate (%)': error_percentage_log_reg,\n",
    "    'Log_reg f1_score (%)': f1_log_reg,\n",
    "    'Log_reg FP (%)' : \"22\"\n",
    "})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DaneshKar\\Project_Eden\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.6077 - loss: 0.6876 - val_accuracy: 0.6354 - val_loss: 0.6913 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7047 - loss: 0.6761 - val_accuracy: 0.6146 - val_loss: 0.6849 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7019 - loss: 0.6599 - val_accuracy: 0.6146 - val_loss: 0.6777 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6951 - loss: 0.6400 - val_accuracy: 0.6146 - val_loss: 0.6692 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7240 - loss: 0.6293 - val_accuracy: 0.6146 - val_loss: 0.6598 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7278 - loss: 0.6064 - val_accuracy: 0.6146 - val_loss: 0.6543 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7195 - loss: 0.5968 - val_accuracy: 0.6146 - val_loss: 0.6541 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7219 - loss: 0.5796 - val_accuracy: 0.6146 - val_loss: 0.6527 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7127 - loss: 0.5610 - val_accuracy: 0.6146 - val_loss: 0.6508 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7221 - loss: 0.5540 - val_accuracy: 0.6146 - val_loss: 0.6474 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7352 - loss: 0.5242 - val_accuracy: 0.6562 - val_loss: 0.6495 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7422 - loss: 0.5349 - val_accuracy: 0.6667 - val_loss: 0.6570 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7654 - loss: 0.5197 - val_accuracy: 0.6875 - val_loss: 0.6532 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7767 - loss: 0.5274 - val_accuracy: 0.7188 - val_loss: 0.6431 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7947 - loss: 0.5450 - val_accuracy: 0.7188 - val_loss: 0.6336 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7905 - loss: 0.5007 - val_accuracy: 0.7188 - val_loss: 0.6295 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8062 - loss: 0.4860 - val_accuracy: 0.7188 - val_loss: 0.6292 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7940 - loss: 0.5001 - val_accuracy: 0.7188 - val_loss: 0.6321 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8187 - loss: 0.4826 - val_accuracy: 0.7188 - val_loss: 0.6272 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8190 - loss: 0.4735 - val_accuracy: 0.7188 - val_loss: 0.6198 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8127 - loss: 0.4758 - val_accuracy: 0.7188 - val_loss: 0.6115 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8018 - loss: 0.4767 - val_accuracy: 0.7188 - val_loss: 0.6147 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8328 - loss: 0.4548 - val_accuracy: 0.7188 - val_loss: 0.6251 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8291 - loss: 0.4637 - val_accuracy: 0.7188 - val_loss: 0.6256 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8041 - loss: 0.4444 - val_accuracy: 0.7188 - val_loss: 0.6146 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7985 - loss: 0.4514 - val_accuracy: 0.7188 - val_loss: 0.6146 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8240 - loss: 0.4352 - val_accuracy: 0.7188 - val_loss: 0.6062 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8361 - loss: 0.4211 - val_accuracy: 0.7188 - val_loss: 0.6032 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8367 - loss: 0.4601 - val_accuracy: 0.7188 - val_loss: 0.5963 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8322 - loss: 0.4407 - val_accuracy: 0.7188 - val_loss: 0.5950 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8318 - loss: 0.4370 - val_accuracy: 0.7188 - val_loss: 0.5976 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8241 - loss: 0.4408 - val_accuracy: 0.7188 - val_loss: 0.5927 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8410 - loss: 0.4353 - val_accuracy: 0.7292 - val_loss: 0.5936 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8408 - loss: 0.4256 - val_accuracy: 0.7292 - val_loss: 0.6013 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8142 - loss: 0.4327 - val_accuracy: 0.7292 - val_loss: 0.6017 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8297 - loss: 0.3944 - val_accuracy: 0.7292 - val_loss: 0.5968 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8375 - loss: 0.3858 - val_accuracy: 0.7292 - val_loss: 0.5941 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8494 - loss: 0.4037 - val_accuracy: 0.7188 - val_loss: 0.5927 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8354 - loss: 0.4141 - val_accuracy: 0.7083 - val_loss: 0.5946 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8333 - loss: 0.4032 - val_accuracy: 0.7188 - val_loss: 0.6035 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8393 - loss: 0.4140 - val_accuracy: 0.7292 - val_loss: 0.6114 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8405 - loss: 0.4043 - val_accuracy: 0.7292 - val_loss: 0.6058 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8433 - loss: 0.3817 - val_accuracy: 0.7292 - val_loss: 0.6056 - learning_rate: 5.0000e-04\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "F1 Score (ANN): 0.8756756756756757\n",
      "Confusion Matrix (ANN):\n",
      " [[16 20]\n",
      " [ 3 81]]\n",
      "False Positives (FP) (ANN): 20\n",
      "False Negatives (FN) (ANN): 3\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import keras\n",
    "keras.utils.set_random_seed(42)\n",
    "# Define the ANN model\n",
    "model_ann = Sequential()\n",
    "model_ann.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model_ann.add(Dropout(0.3))\n",
    "model_ann.add(Dense(32, activation='relu'))\n",
    "model_ann.add(Dropout(0.3))\n",
    "model_ann.add(Dense(16, activation='relu'))\n",
    "model_ann.add(Dropout(0.2))\n",
    "model_ann.add(Dense(8, activation='relu'))\n",
    "model_ann.add(Dropout(0.2))\n",
    "model_ann.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early Stopping برای جلوگیری از Overfitting\n",
    "early_stopping = EarlyStopping(monitor='accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# ReduceLROnPlateau برای کاهش نرخ یادگیری به صورت پویا\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001)\n",
    "\n",
    "#  Train the model\n",
    "history = model_ann.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.2,callbacks=[early_stopping, lr_scheduler], verbose=1)\n",
    "\n",
    "# Predictions and threshold adjustment\n",
    "y_pred_ann = (model_ann.predict(X_test) > 0.5).astype('int32')\n",
    "\n",
    "#  Evaluation\n",
    "f1_ann = f1_score(y_test, y_pred_ann)\n",
    "conf_matrix_ann = confusion_matrix(y_test, y_pred_ann)\n",
    "\n",
    "print(\"F1 Score (ANN):\", f1_ann)\n",
    "print(\"Confusion Matrix (ANN):\\n\", conf_matrix_ann)\n",
    "\n",
    "fp_ann = conf_matrix_ann[0, 1]\n",
    "fn_ann = conf_matrix_ann[1, 0]\n",
    "print(f\"False Positives (FP) (ANN): {fp_ann}\")\n",
    "print(f\"False Negatives (FN) (ANN): {fn_ann}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (ANN): 0.8756756756756757\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score (ANN):\", f1_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Error: 19.166666666666664 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_ann)\n",
    "error_percentage_ann = 100 * (1 - accuracy)\n",
    "print(\"Percentage of Error:\", error_percentage_ann, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list.append({\n",
    "    'Ann Error Rate (%)': error_percentage_ann,\n",
    "    'Ann f1_score  (%)': f1_ann,\n",
    "    'Ann FP (%)' : \"20\"\n",
    "})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Random Forest Error Rate (%)': 23.33333333333333,\n",
       "  'Random Forest f1-score (%)': 0.8372093023255814,\n",
       "  'Random Forest FP (%)': '16'},\n",
       " {'XGBOOST Error Rate (%)': 25.0,\n",
       "  'XGBOOST f1-score (%)': 0.8235294117647058,\n",
       "  'XGBOOST FP (%)': '16'},\n",
       " {'Log_reg Error Rate (%)': 20.833333333333336,\n",
       "  'Log_reg f1_score (%)': 0.861878453038674,\n",
       "  'Log_reg FP (%)': '19'},\n",
       " {'Log_reg Error Rate (%)': 20.833333333333336,\n",
       "  'Log_reg f1_score (%)': 0.861878453038674,\n",
       "  'Log_reg FP (%)': '22'},\n",
       " {'Ann Error Rate (%)': 19.166666666666664,\n",
       "  'Ann f1_score  (%)': 0.8756756756756757,\n",
       "  'Ann FP (%)': '20'}]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine results into a DataFrame.\n",
    "\n",
    "\n",
    "results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Model Evaluation and Recommendation\n",
    "\n",
    "### Summary of Model Metrics:\n",
    "\n",
    "| **Model**      | **Error Rate (%)** | **F1 Score (%)** | **False Positives (FP)** |\n",
    "|----------------|--------------------|------------------|--------------------------|\n",
    "| **Random Forest** | 23.33             | 83.72            | 16                       |\n",
    "| **XGBoost**      | 25.00             | 82.35            | 16                       |\n",
    "| **Logistic Regression (1)**  | 20.83             | 86.19            | 19                       |\n",
    "| **Logistic Regression (2)**  | 20.83             | 86.19            | 22                       |\n",
    "| **ANN**           | 19.17             | 87.57            | 20                       |\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis and Conclusion:\n",
    "\n",
    "### Key Insights:\n",
    "1. **Best F1 Score:**\n",
    "   - **ANN** has the highest F1 score at **87.57%**, indicating excellent balance between Precision and Recall.\n",
    "\n",
    "2. **Best Error Rate:**\n",
    "   - **ANN** also has the lowest **error rate** at **19.17%**, showing superior accuracy compared to all other models.\n",
    "\n",
    "3. **False Positives:**\n",
    "   - **XGBoost** and **Random Forest** have the **lowest False Positives (FP)** at **16**, which is preferable in scenarios where minimizing false alarms is critical.\n",
    "   - However, they come with a **higher error rate** and slightly lower F1 scores compared to **ANN**.\n",
    "\n",
    "4. **Logistic Regression Performance:**\n",
    "   - The two Logistic Regression models perform similarly, with an F1 score of **86.19%** and an error rate of **20.83%**, but they have relatively higher False Positives compared to the other models, especially with **22 FP** in the second logistic regression model.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Recommendation:\n",
    "- **ANN** is the best overall model based on both **F1 Score** and **Error Rate**. Although its False Positives are slightly higher (**20 FP**) compared to **XGBoost** and **Random Forest**, its high accuracy and F1 score make it the strongest candidate.\n",
    "- If minimizing **False Positives (FP)** is the absolute priority, **Random Forest** and **XGBoost** are preferable due to their lowest FP values (**16**).\n",
    "- However, **ANN** provides the best tradeoff between accuracy, F1 score, and error rate, making it the recommended model if you can tolerate slightly higher False Positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Output as CSV\n",
    "     Predicted based on best of the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "   Dependents  Education  Credit_History  Property_Area  Gender_Male  \\\n",
      "0           0          1             1.0              2            1   \n",
      "1           1          1             1.0              2            1   \n",
      "2           2          1             1.0              2            1   \n",
      "3           2          1             1.0              2            1   \n",
      "4           0          0             1.0              2            1   \n",
      "\n",
      "   Married_Yes  Self_Employed_Yes  Log_ApplicantIncome  Log_CoapplicantIncome  \\\n",
      "0            1                  0             8.651899               0.000000   \n",
      "1            1                  0             8.031710               7.313887   \n",
      "2            1                  0             8.517393               7.496097   \n",
      "3            1                  0             7.758333               7.842671   \n",
      "4            0                  0             8.094684               0.000000   \n",
      "\n",
      "   Log_LoanAmount  BoxCox_Loan_Amount_Term  Loan_Status_pred  \n",
      "0        4.709530             1.757278e+07                 1  \n",
      "1        4.844187             1.757278e+07                 1  \n",
      "2        5.342334             1.757278e+07                 1  \n",
      "3        4.615121             1.757278e+07                 1  \n",
      "4        4.369448             1.757278e+07                 1  \n"
     ]
    }
   ],
   "source": [
    "# پیش‌بینی‌های احتمال از مدل ANN\n",
    "y_test_pred_prob = model_ann.predict(X_test_scaled)\n",
    "\n",
    "# تبدیل پیش‌بینی‌ها به صفر و یک با استفاده از آستانه 0.5\n",
    "y_test_pred_binary = (y_test_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# اضافه کردن پیش‌بینی‌های باینری به داده‌های تست\n",
    "test_data['Loan_Status_pred'] = y_test_pred_binary  # پیش‌بینی‌های باینری در ستون جدید\n",
    "\n",
    "# ذخیره داده‌های به‌روزرسانی شده در یک فایل جدید\n",
    "test_data.to_csv(r'd:\\DaneshKar\\Project_Eden\\Clean_Test_Updated_best.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Married_Yes</th>\n",
       "      <th>Self_Employed_Yes</th>\n",
       "      <th>Log_ApplicantIncome</th>\n",
       "      <th>Log_CoapplicantIncome</th>\n",
       "      <th>Log_LoanAmount</th>\n",
       "      <th>BoxCox_Loan_Amount_Term</th>\n",
       "      <th>Loan_Status_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.651899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.709530</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.031710</td>\n",
       "      <td>7.313887</td>\n",
       "      <td>4.844187</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.517393</td>\n",
       "      <td>7.496097</td>\n",
       "      <td>5.342334</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.758333</td>\n",
       "      <td>7.842671</td>\n",
       "      <td>4.615121</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.094684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.369448</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.680637</td>\n",
       "      <td>8.138273</td>\n",
       "      <td>5.030438</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.708411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.094345</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.264106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.997212</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.520322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.638355</td>\n",
       "      <td>5.185310e+06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.783641</td>\n",
       "      <td>7.783641</td>\n",
       "      <td>4.820282</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.036573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.510860</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.689829</td>\n",
       "      <td>7.324490</td>\n",
       "      <td>5.093750</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.334952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.713572</td>\n",
       "      <td>2.183796e+06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.407058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.117994</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.448272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.828314</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.642592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.882802</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.430327</td>\n",
       "      <td>7.978311</td>\n",
       "      <td>5.303305</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.239329</td>\n",
       "      <td>5.811141</td>\n",
       "      <td>4.844187</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.129889</td>\n",
       "      <td>8.976768</td>\n",
       "      <td>5.707110</td>\n",
       "      <td>1.757278e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>8.152198</td>\n",
       "      <td>4.615121</td>\n",
       "      <td>2.183796e+06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dependents  Education  Credit_History  Property_Area  Gender_Male  \\\n",
       "0            0          1             1.0              2            1   \n",
       "1            1          1             1.0              2            1   \n",
       "2            2          1             1.0              2            1   \n",
       "3            2          1             1.0              2            1   \n",
       "4            0          0             1.0              2            1   \n",
       "5            0          0             1.0              2            1   \n",
       "6            1          0             1.0              1            0   \n",
       "7            2          0             0.0              0            1   \n",
       "8            2          1             1.0              2            1   \n",
       "9            0          0             1.0              1            1   \n",
       "10           0          0             1.0              2            1   \n",
       "11           1          1             1.0              1            1   \n",
       "12           3          1             1.0              2            1   \n",
       "13           2          1             0.0              1            1   \n",
       "14           0          1             1.0              1            0   \n",
       "15           1          1             1.0              2            1   \n",
       "16           2          1             1.0              2            1   \n",
       "17           3          1             1.0              1            1   \n",
       "18           0          1             1.0              2            1   \n",
       "19           0          1             1.0              1            1   \n",
       "\n",
       "    Married_Yes  Self_Employed_Yes  Log_ApplicantIncome  \\\n",
       "0             1                  0             8.651899   \n",
       "1             1                  0             8.031710   \n",
       "2             1                  0             8.517393   \n",
       "3             1                  0             7.758333   \n",
       "4             0                  0             8.094684   \n",
       "5             1                  1             7.680637   \n",
       "6             0                  0             7.708411   \n",
       "7             1                  0             8.264106   \n",
       "8             1                  0             9.520322   \n",
       "9             0                  0             7.783641   \n",
       "10            0                  0             8.036573   \n",
       "11            1                  0             7.689829   \n",
       "12            0                  0             8.334952   \n",
       "13            1                  0             9.407058   \n",
       "14            0                  0             8.448272   \n",
       "15            0                  0             8.642592   \n",
       "16            1                  0             8.430327   \n",
       "17            1                  0             8.239329   \n",
       "18            1                  0             9.129889   \n",
       "19            0                  0             7.170888   \n",
       "\n",
       "    Log_CoapplicantIncome  Log_LoanAmount  BoxCox_Loan_Amount_Term  \\\n",
       "0                0.000000        4.709530             1.757278e+07   \n",
       "1                7.313887        4.844187             1.757278e+07   \n",
       "2                7.496097        5.342334             1.757278e+07   \n",
       "3                7.842671        4.615121             1.757278e+07   \n",
       "4                0.000000        4.369448             1.757278e+07   \n",
       "5                8.138273        5.030438             1.757278e+07   \n",
       "6                0.000000        4.094345             1.757278e+07   \n",
       "7                0.000000        4.997212             1.757278e+07   \n",
       "8                0.000000        5.638355             5.185310e+06   \n",
       "9                7.783641        4.820282             1.757278e+07   \n",
       "10               0.000000        4.510860             1.757278e+07   \n",
       "11               7.324490        5.093750             1.757278e+07   \n",
       "12               0.000000        3.713572             2.183796e+06   \n",
       "13               0.000000        5.117994             1.757278e+07   \n",
       "14               0.000000        4.828314             1.757278e+07   \n",
       "15               0.000000        4.882802             1.757278e+07   \n",
       "16               7.978311        5.303305             1.757278e+07   \n",
       "17               5.811141        4.844187             1.757278e+07   \n",
       "18               8.976768        5.707110             1.757278e+07   \n",
       "19               8.152198        4.615121             2.183796e+06   \n",
       "\n",
       "    Loan_Status_pred  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  \n",
       "5                  1  \n",
       "6                  1  \n",
       "7                  0  \n",
       "8                  1  \n",
       "9                  1  \n",
       "10                 1  \n",
       "11                 1  \n",
       "12                 1  \n",
       "13                 0  \n",
       "14                 1  \n",
       "15                 1  \n",
       "16                 1  \n",
       "17                 1  \n",
       "18                 1  \n",
       "19                 1  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
